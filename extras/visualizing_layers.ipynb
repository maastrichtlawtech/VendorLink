{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe530f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from torch import cuda\n",
    "from tqdm import tqdm \n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# from flair.data import Sentence\n",
    "# from flair.models import SequenceTagger\n",
    "\n",
    "# import spacy\n",
    "\n",
    "# from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from transformers import AutoModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# from transformers_interpret import SequenceClassificationExplainer\n",
    "from transformers import BertTokenizer, BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "\n",
    "# %% Loading custom libraries \n",
    "sys.path.append('../metrics/')\n",
    "from performance import f1_score_func, accuracy_per_class\n",
    "\n",
    "# Loading the custom library\n",
    "sys.path.append('../process/')\n",
    "from load_data import FetchData, ContextualizedData\n",
    "from utils import merge_and_create_dataframe, train_model, evaluate_model, clean_and_merge_data_for_tokenization, add_tokens_to_vocabulary\n",
    "\n",
    "# from captum.attr import visualization as viz\n",
    "# from captum.attr import LayerConductance, LayerIntegratedGradients\n",
    "\n",
    "# from scipy.special import softmax\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6248a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data='alpha-dreams' ,\n",
    "args = dict(ads_count=20,\n",
    " batch_size=64,\n",
    " cuda=False,\n",
    " data='alpha-dreams', \n",
    " data_dir='../data',\n",
    " delta=0.01,\n",
    " dropout=0.65,\n",
    " early_stopping=True,\n",
    " eval_per_steps=2000,\n",
    " hidden_states=512,\n",
    " load_model='epoch_38.model',\n",
    " lr=4e-05,\n",
    " max_seq_len=512,\n",
    " mode='train',\n",
    " model='bert',\n",
    " n_splits=5,\n",
    " nb_epochs=10,\n",
    " patience=3,\n",
    " preprocess_flag=False,\n",
    " save_dir='../models/merged',\n",
    " seed=1111,\n",
    " setting='high',\n",
    " split_ratio=0.25,\n",
    " version='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66341bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Loading the datasets\n",
    "alpha_df = pd.read_csv(os.path.join(args['data_dir'], \"preprocessed_alpha.csv\"), error_bad_lines=False, \n",
    "                            lineterminator='\\n', usecols=['marketplace', 'title', 'vendor', 'prediction', 'ships_to', 'ships_from', 'description']).drop_duplicates()\n",
    "dreams_df = pd.read_csv(os.path.join(args['data_dir'], \"preprocessed_dreams.csv\"), error_bad_lines=False, \n",
    "                            lineterminator='\\n', usecols=['marketplace', 'title', 'vendor', 'prediction', 'ships_to', 'ships_from', 'description']).drop_duplicates()\n",
    "silk_df = pd.read_csv(os.path.join(args['data_dir'], \"preprocessed_silk.csv\"), error_bad_lines=False, \n",
    "                            lineterminator='\\n', usecols=['marketplace', 'title', 'vendor', 'prediction', 'ships_to', 'ships_from', 'description']).drop_duplicates()\n",
    "data_df = {\"alpha\":alpha_df, \"dreams\":dreams_df, \"silk\":silk_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42c8160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the data ...\n",
      "Training and Test data size for Alphabay market : (75327, 7) (25109, 7)\n",
      "Training and Test data size for Dreams market : (70198, 7) (23400, 7)\n",
      "Training and Test data size for Silk-Road market : (89544, 7) (29848, 7)\n",
      "Splitting combined data ...\n"
     ]
    }
   ],
   "source": [
    "[(train_alpha_dreams, train_dreams_silk, train_alpha_silk, train_alpha_dreams_silk, train_alpha, train_dreams, train_silk), (test_alpha_dreams, test_dreams_silk, test_alpha_silk, test_alpha_dreams_silk, test_alpha, test_dreams, test_silk)] = FetchData(data_df, args[\"version\"], args[\"data\"],  args[\"split_ratio\"], args[\"preprocess_flag\"], args[\"setting\"], args[\"ads_count\"],  args[\"seed\"]).split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86801db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the data ...\n",
      "Training and Test data size for Valhalla market : (1631, 7) (544, 7)\n",
      "Training and Test data size for Traderoute market : (14964, 7) (4989, 7)\n",
      "Training and Test data size for Berlusconi market : (1077, 7) (360, 7)\n",
      "Splitting combined data ...\n"
     ]
    }
   ],
   "source": [
    "[(train_valhalla_traderoute, train_traderoute_berlusconi, train_valhalla_berlusconi, train_valhalla_traderoute_berlusconi, train_valhalla, train_traderoute, train_berlusconi),\n",
    "                        (test_valhalla_traderoute, test_traderoute_berlusconi, test_valhalla_berlusconi, test_valhalla_traderoute_berlusconi, test_valhalla, test_traderoute, test_berlusconi)] = FetchData(data_df, args[\"version\"], args[\"data\"],  args[\"split_ratio\"], args[\"preprocess_flag\"], args[\"setting\"], args[\"ads_count\"],  args[\"seed\"]).split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebbcc6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_dreams, test_dreams, train_alpha, test_alpha, train_silk, test_silk])\n",
    "all_vendors = list(data['vendor'].unique())\n",
    "vendor_to_idx_dict = {vendor_name:index for index, vendor_name in enumerate(all_vendors)}\n",
    "# data['vendor'] = data['vendor'].replace(vendor_to_idx_dict, regex=True)\n",
    "# sample_df = merge_and_create_dataframe(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "167db1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = merge_and_create_dataframe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fd719ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', truncation=True, do_lower_case=True)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                            num_labels=len(vendor_to_idx_dict),\n",
    "                                            output_attentions=False,\n",
    "                                            output_hidden_states=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "844ed18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model.load_state_dict(torch.load(os.path.join(args['save_dir'], args['model'], args['load_model'])))\n",
    "model.eval()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdf5f3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=3896, bias=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dbc510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517ff87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35408ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3f409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7e417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2adc046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8d2d0c",
   "metadata": {},
   "source": [
    "# Helper class to collect the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b16ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_layer_representations(model, dataloader_, vendor_to_idx_dict, device, mode='all'):    \n",
    "    # Initializing the layers with zero activation with (layer, mean embedding)\n",
    "    layer_representations = torch.zeros(13,768)\n",
    "    concat_hidden_repr = []\n",
    "    vendor_dict = {}\n",
    "    model.eval()\n",
    "    for batch in dataloader_:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        vendor_id = int(batch[2].unique()[0].detach().cpu())\n",
    "        vendor_name = list(vendor_to_idx_dict.keys())[list(vendor_to_idx_dict.values()).index(vendor_id)]\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            hidden_repr = outputs['hidden_states']\n",
    "            # Stacking all the layers of the model\n",
    "            hidden_repr = torch.stack(hidden_repr, dim=0).detach().cpu()\n",
    "            # Concatinating all batches\n",
    "            concat_hidden_repr.append(hidden_repr)\n",
    "     \n",
    "    # flattening all the batches\n",
    "    hidden_repr = torch.cat(concat_hidden_repr, dim=1)\n",
    "    no_of_layers, batch_size, sequence_size, hidden_units = hidden_repr.size()\n",
    "    if mode != 'all':\n",
    "        for layer in range(no_of_layers):\n",
    "            for sequence in range(batch_size):\n",
    "                # Only considering the embeddings from the cls token\n",
    "                layer_representations[layer] += hidden_repr[layer,sequence,0,:].detach().cpu()                    \n",
    "\n",
    "    else:\n",
    "        for layer in range(no_of_layers):\n",
    "            for sequence in range(batch_size):\n",
    "                # Considering mean of embeddings of all the layers\n",
    "                temp_hidden_repr = torch.mean(hidden_repr[layer,sequence,:,:], dim=0).detach().cpu() \n",
    "                layer_representations[layer] += temp_hidden_repr\n",
    "\n",
    "    layer_representations[layer] /= batch_size\n",
    "    return layer_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6fc1bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_layer_representations_from_last_layer(model, dataloader_, attention_mask, vendor_to_idx_dict, device):    \n",
    "    # Initializing the layers with zero activation with (layer, mean embedding)\n",
    "    \n",
    "    concat_hidden_repr = []\n",
    "    model.eval()\n",
    "    for batch in dataloader_:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        vendor_id = int(batch[2].unique()[0].detach().cpu())\n",
    "        vendor_name = list(vendor_to_idx_dict.keys())[list(vendor_to_idx_dict.values()).index(vendor_id)]\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            hidden_repr = outputs['hidden_states']\n",
    "            # Stacking all the layers of the model\n",
    "            hidden_repr = torch.stack(hidden_repr, dim=0).detach().cpu()\n",
    "            # Concatinating all batches\n",
    "            concat_hidden_repr.append(hidden_repr)\n",
    "     \n",
    "    # flattening all the batches\n",
    "    hidden_repr = torch.cat(concat_hidden_repr, dim=1)[-1]\n",
    "    # resizing the attention_mask tensor\n",
    "    mask = attention_mask.unsqueeze(-1).expand(hidden_repr.size()).float()\n",
    "    # Each vector above represents a single token attention mask - each token now has a vector of size 768 representing it's attention_mask status. Then we multiply the two tensors to apply the attention mask\n",
    "    masked_embeddings = hidden_repr * mask\n",
    "    # Summing the remained of the embeddings along axis 1\n",
    "    summed = torch.sum(masked_embeddings, 1)\n",
    "    # Sum the number of values that must be given attention in each position of the tensors\n",
    "    summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "    # we calculate the mean as the sum of the embedding activations summed divided by the number of values that should be given attention in each position summed_mask\n",
    "    mean_pooled = summed / summed_mask\n",
    "\n",
    "    return mean_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a501bc9",
   "metadata": {},
   "source": [
    "# Original Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3dbcb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = data[data['labels']==3]\n",
    "# Encoding the data through the transformer tokenizer\n",
    "encoded_data = tokenizer.batch_encode_plus(sample_df.text.values, \n",
    "                                           add_special_tokens=True, \n",
    "                                           return_attention_mask=True, \n",
    "                                           pad_to_max_length=True, \n",
    "                                           max_length=512, \n",
    "                                           return_tensors='pt')\n",
    "\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "\n",
    "labels_ = torch.tensor(sample_df.labels.values)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels_)\n",
    "\n",
    "# Data Loaders\n",
    "# We use RandomSampler for training and SequentialSampler for testing and validation\n",
    "dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=32)\n",
    "hidden_representation = extract_layer_representations_from_last_layer(model, dataloader, attention_masks, vendor_to_idx_dict, device, mode='cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac94995",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vendor_id = list(data['labels'].unique())\n",
    "unique_vendor_id.remove(vendor_to_idx_dict['others'])\n",
    "\n",
    "pbar = tqdm(total=len(unique_vendor_id))\n",
    "vendor_representation_dict = {}\n",
    "for vendor in unique_vendor_id:\n",
    "    sample_df = data[data['labels']==vendor]\n",
    "    # Encoding the data through the transformer tokenizer\n",
    "    encoded_data = tokenizer.batch_encode_plus(sample_df.text.values, \n",
    "                                               add_special_tokens=True, \n",
    "                                               return_attention_mask=True, \n",
    "                                               pad_to_max_length=True, \n",
    "                                               max_length=512, \n",
    "                                               return_tensors='pt')\n",
    "    \n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_masks = encoded_data['attention_mask']\n",
    "    labels_ = torch.tensor(sample_df.labels.values)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels_)\n",
    "    \n",
    "    # Data Loaders\n",
    "    # We use RandomSampler for training and SequentialSampler for testing and validation\n",
    "    dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=32)\n",
    "    hidden_representation = extract_layer_representations(model, dataloader, vendor_to_idx_dict, device)\n",
    "    vendor_name = list(vendor_to_idx_dict.keys())[list(vendor_to_idx_dict.values()).index(vendor)]\n",
    "    vendor_representation_dict[vendor_name] = hidden_representation\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "add1566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pickled/bert_all_representations_after_training.pickle', 'wb') as handle:\n",
    "    pickle.dump(vendor_representation_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c848c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3605 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████| 3605/3605 [2:11:04<00:00,  2.18s/it]  \n"
     ]
    }
   ],
   "source": [
    "unique_vendor_id = list(data['labels'].unique())\n",
    "unique_vendor_id.remove(vendor_to_idx_dict['others'])\n",
    "\n",
    "pbar = tqdm(total=len(unique_vendor_id))\n",
    "vendor_representation_dict = {}\n",
    "for vendor in unique_vendor_id:\n",
    "    sample_df = data[data['labels']==vendor]\n",
    "    # Encoding the data through the transformer tokenizer\n",
    "    encoded_data = tokenizer.batch_encode_plus(sample_df.text.values, \n",
    "                                               add_special_tokens=True, \n",
    "                                               return_attention_mask=True, \n",
    "                                               pad_to_max_length=True, \n",
    "                                               max_length=512, \n",
    "                                               return_tensors='pt')\n",
    "    \n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_masks = encoded_data['attention_mask']\n",
    "    labels_ = torch.tensor(sample_df.labels.values)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels_)\n",
    "    \n",
    "    # Data Loaders\n",
    "    # We use RandomSampler for training and SequentialSampler for testing and validation\n",
    "    dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=32)\n",
    "    hidden_representation = extract_layer_representations(model, dataloader, vendor_to_idx_dict, device, mode='cls')\n",
    "    vendor_name = list(vendor_to_idx_dict.keys())[list(vendor_to_idx_dict.values()).index(vendor)]\n",
    "    vendor_representation_dict[vendor_name] = hidden_representation\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9998931",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pickled/bert_cls_representations_after_training.pickle', 'wb') as handle:\n",
    "    pickle.dump(vendor_representation_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947a5bb",
   "metadata": {},
   "source": [
    "# Loading the pickled files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2da9b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f9a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pickled/bert_cls_representations_before_training.pickle', 'rb') as handle:\n",
    "    cls_before = pickle.load(handle)\n",
    "    \n",
    "with open('../data/pickled/bert_cls_representations_after_training.pickle', 'rb') as handle:\n",
    "    cls_after = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "823d6da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pickled/bert_all_representations_before_training.pickle', 'rb') as handle:\n",
    "    cls_before = pickle.load(handle)\n",
    "    \n",
    "with open('../data/pickled/bert_all_representations_after_training.pickle', 'rb') as handle:\n",
    "    cls_after = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92a48b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_before_mean = torch.mean(torch.stack(list(cls_before.values())), 0)\n",
    "cls_after_mean = torch.mean(torch.stack(list(cls_after.values())), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196e565",
   "metadata": {},
   "source": [
    "# CKA - Center Kernel Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ff3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ca9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by\n",
    "# https://github.com/yuanli2333/CKA-Centered-Kernel-Alignment/blob/master/CKA.py\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CKA(object):\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    \n",
    "    def centering(self, K):\n",
    "        n = K.shape[0]\n",
    "        unit = np.ones([n, n])\n",
    "        I = np.eye(n)\n",
    "        H = I - unit / n\n",
    "        return np.dot(np.dot(H, K), H) \n",
    "\n",
    "    def rbf(self, X, sigma=None):\n",
    "        GX = np.dot(X, X.T)\n",
    "        KX = np.diag(GX) - GX + (np.diag(GX) - GX).T\n",
    "        if sigma is None:\n",
    "            mdist = np.median(KX[KX != 0])\n",
    "            sigma = math.sqrt(mdist)\n",
    "        KX *= - 0.5 / (sigma * sigma)\n",
    "        KX = np.exp(KX)\n",
    "        return KX\n",
    " \n",
    "    def kernel_HSIC(self, X, Y, sigma):\n",
    "        return np.sum(self.centering(self.rbf(X, sigma)) * self.centering(self.rbf(Y, sigma)))\n",
    "\n",
    "    def linear_HSIC(self, X, Y):\n",
    "        L_X = X @ X.T\n",
    "        L_Y = Y @ Y.T\n",
    "        return np.sum(self.centering(L_X) * self.centering(L_Y))\n",
    "\n",
    "    def linear_CKA(self, X, Y):\n",
    "        hsic = self.linear_HSIC(X, Y)\n",
    "        var1 = np.sqrt(self.linear_HSIC(X, X))\n",
    "        var2 = np.sqrt(self.linear_HSIC(Y, Y))\n",
    "\n",
    "        return hsic / (var1 * var2)\n",
    "\n",
    "    def kernel_CKA(self, X, Y, sigma=None):\n",
    "        hsic = self.kernel_HSIC(X, Y, sigma)\n",
    "        var1 = np.sqrt(self.kernel_HSIC(X, X, sigma))\n",
    "        var2 = np.sqrt(self.kernel_HSIC(Y, Y, sigma))\n",
    "\n",
    "        return hsic / (var1 * var2)\n",
    "\n",
    "    \n",
    "class CudaCKA(object):\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "    \n",
    "    def centering(self, K):\n",
    "        n = K.shape[0]\n",
    "        unit = torch.ones([n, n], device=self.device)\n",
    "        I = torch.eye(n, device=self.device)\n",
    "        H = I - unit / n\n",
    "        return torch.matmul(torch.matmul(H, K), H)  \n",
    "\n",
    "    def rbf(self, X, sigma=None):\n",
    "        GX = torch.matmul(X, X.T)\n",
    "        KX = torch.diag(GX) - GX + (torch.diag(GX) - GX).T\n",
    "        if sigma is None:\n",
    "            mdist = torch.median(KX[KX != 0])\n",
    "            sigma = math.sqrt(mdist)\n",
    "        KX *= - 0.5 / (sigma * sigma)\n",
    "        KX = torch.exp(KX)\n",
    "        return KX\n",
    "\n",
    "    def kernel_HSIC(self, X, Y, sigma):\n",
    "        return torch.sum(self.centering(self.rbf(X, sigma)) * self.centering(self.rbf(Y, sigma)))\n",
    "\n",
    "    def linear_HSIC(self, X, Y):\n",
    "        L_X = torch.matmul(X, X.T)\n",
    "        L_Y = torch.matmul(Y, Y.T)\n",
    "        return torch.sum(self.centering(L_X) * self.centering(L_Y))\n",
    "\n",
    "    def linear_CKA(self, X, Y):\n",
    "        hsic = self.linear_HSIC(X, Y)\n",
    "        var1 = torch.sqrt(self.linear_HSIC(X, X))\n",
    "        var2 = torch.sqrt(self.linear_HSIC(Y, Y))\n",
    "\n",
    "        return hsic / (var1 * var2)\n",
    "\n",
    "    def kernel_CKA(self, X, Y, sigma=None):\n",
    "        hsic = self.kernel_HSIC(X, Y, sigma)\n",
    "        var1 = torch.sqrt(self.kernel_HSIC(X, X, sigma))\n",
    "        var2 = torch.sqrt(self.kernel_HSIC(Y, Y, sigma))\n",
    "        return hsic / (var1 * var2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2046abc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import math\\nimport numpy as np\\n\\n\\ndef centering(K):\\n    n = K.shape[0]\\n    unit = np.ones([n, n])\\n    I = np.eye(n)\\n    H = I - unit / n\\n\\n    return np.dot(np.dot(H, K), H)  # HKH are the same with KH, KH is the first centering, H(KH) do the second time, results are the sme with one time centering\\n    # return np.dot(H, K)  # KH\\n\\n\\ndef rbf(X, sigma=None):\\n    GX = np.dot(X, X.T)\\n    KX = np.diag(GX) - GX + (np.diag(GX) - GX).T\\n    if sigma is None:\\n        mdist = np.median(KX[KX != 0])\\n        sigma = math.sqrt(mdist)\\n    KX *= - 0.5 / (sigma * sigma)\\n    KX = np.exp(KX)\\n    return KX\\n\\n\\ndef kernel_HSIC(X, Y, sigma):\\n    return np.sum(centering(rbf(X, sigma)) * centering(rbf(Y, sigma)))\\n\\n\\ndef linear_HSIC(X, Y):\\n    L_X = np.dot(X, X.T)\\n    L_Y = np.dot(Y, Y.T)\\n    return np.sum(centering(L_X) * centering(L_Y))\\n\\n\\ndef linear_CKA(X, Y):\\n    hsic = linear_HSIC(X, Y)\\n    var1 = np.sqrt(linear_HSIC(X, X))\\n    var2 = np.sqrt(linear_HSIC(Y, Y))\\n\\n    return hsic / (var1 * var2)\\n\\n\\ndef kernel_CKA(X, Y, sigma=None):\\n    hsic = kernel_HSIC(X, Y, sigma)\\n    var1 = np.sqrt(kernel_HSIC(X, X, sigma))\\n    var2 = np.sqrt(kernel_HSIC(Y, Y, sigma))\\n\\n    return hsic / (var1 * var2)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def centering(K):\n",
    "    n = K.shape[0]\n",
    "    unit = np.ones([n, n])\n",
    "    I = np.eye(n)\n",
    "    H = I - unit / n\n",
    "\n",
    "    return np.dot(np.dot(H, K), H)  # HKH are the same with KH, KH is the first centering, H(KH) do the second time, results are the sme with one time centering\n",
    "    # return np.dot(H, K)  # KH\n",
    "\n",
    "\n",
    "def rbf(X, sigma=None):\n",
    "    GX = np.dot(X, X.T)\n",
    "    KX = np.diag(GX) - GX + (np.diag(GX) - GX).T\n",
    "    if sigma is None:\n",
    "        mdist = np.median(KX[KX != 0])\n",
    "        sigma = math.sqrt(mdist)\n",
    "    KX *= - 0.5 / (sigma * sigma)\n",
    "    KX = np.exp(KX)\n",
    "    return KX\n",
    "\n",
    "\n",
    "def kernel_HSIC(X, Y, sigma):\n",
    "    return np.sum(centering(rbf(X, sigma)) * centering(rbf(Y, sigma)))\n",
    "\n",
    "\n",
    "def linear_HSIC(X, Y):\n",
    "    L_X = np.dot(X, X.T)\n",
    "    L_Y = np.dot(Y, Y.T)\n",
    "    return np.sum(centering(L_X) * centering(L_Y))\n",
    "\n",
    "\n",
    "def linear_CKA(X, Y):\n",
    "    hsic = linear_HSIC(X, Y)\n",
    "    var1 = np.sqrt(linear_HSIC(X, X))\n",
    "    var2 = np.sqrt(linear_HSIC(Y, Y))\n",
    "\n",
    "    return hsic / (var1 * var2)\n",
    "\n",
    "\n",
    "def kernel_CKA(X, Y, sigma=None):\n",
    "    hsic = kernel_HSIC(X, Y, sigma)\n",
    "    var1 = np.sqrt(kernel_HSIC(X, X, sigma))\n",
    "    var2 = np.sqrt(kernel_HSIC(Y, Y, sigma))\n",
    "\n",
    "    return hsic / (var1 * var2)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "429117ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004316700149757556"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_cka = CKA()\n",
    "np_cka.linear_CKA(cls_before_mean[-1].unsqueeze(1), cls_after_mean[-1].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "823a2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_cka = CKA()\n",
    "outer_list = []\n",
    "for i in range(cls_after_mean.shape[0]):\n",
    "    inner_list = []\n",
    "    for j in range(cls_before_mean.shape[0]):\n",
    "        inner_list.append(np_cka.linear_CKA(cls_before_mean[j].unsqueeze(1), cls_after_mean[i].unsqueeze(1)))\n",
    "    outer_list.append(inner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92f15221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# create some random data; replace that by your actual dataset\n",
    "data = pd.DataFrame(np.array(outer_list), columns=range(13), index = range(13))\n",
    "\n",
    "# plot heatmap\n",
    "ax = sns.heatmap(data.T)\n",
    "\n",
    "# turn the axis label\n",
    "for item in ax.get_yticklabels():\n",
    "    item.set_rotation(0)\n",
    "\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "\n",
    "# save figure\n",
    "# plt.savefig('seabornPandas.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b57f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import pandas as pd\n",
    "\n",
    "df = px.data.stocks()#.tail(50)\n",
    "df = df.drop(['date'], axis = 1)\n",
    "dfc = df.corr()\n",
    "z = dfc.values.tolist()\n",
    "z = similarity\n",
    "\n",
    "# change each element of z to type string for annotations\n",
    "# z_text = [[str(y) for y in x] for x in z]\n",
    "z_text = [[str(round(y, 4)) for y in x] for x in z]\n",
    "\n",
    "# set up figure \n",
    "fig = ff.create_annotated_heatmap(z, x=list(range(13)),\n",
    "                                     y=list(range(13)),\n",
    "                                     annotation_text=z_text, colorscale='agsunset')\n",
    "\n",
    "# add title\n",
    "fig.update_layout(title_text='<i><b>CKA Similarity before and after finetuning BERT layers</b></i>',\n",
    "                  #xaxis = dict(title='x'),\n",
    "                  #yaxis = dict(title='x')\n",
    "                 )\n",
    "\n",
    "# add custom xaxis title\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=0.5,\n",
    "                        y=-0.15,\n",
    "                        showarrow=False,\n",
    "                        text=\"\",\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeccdb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d15629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c475683e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adceb6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bca01a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b31195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "with open('../data/pickled/bert_sentence_representations_for_vendors.pickle', 'rb') as handle:\n",
    "    sentence_representation = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a84b7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vendor_name = list(sentence_representation.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2f2e3c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(504, 384)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(sentence_representation['debuyerking'],\n",
    "    sentence_representation['drunkninja']\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "54659d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([504, 768]), torch.Size([384, 768]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_representation['debuyerking'].shape, sentence_representation['drunkninja'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a24efea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7175923"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(sentence_representation['planet-pluto'],\n",
    "    sentence_representation['planet-pluto']\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfcda17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3605/3605 [5:49:06<00:00,  5.81s/it]  \n"
     ]
    }
   ],
   "source": [
    "vendor_dict = {}\n",
    "pbar = tqdm(total=len(vendor_name))\n",
    "\n",
    "for vendor1 in vendor_name:\n",
    "    for vendor2 in vendor_name:\n",
    "        vendor_dict[(vendor1, vendor2)] = cosine_similarity(\n",
    "            sentence_representation[vendor1], sentence_representation[vendor2]).mean()\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "535f2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pickled/cosine_similarity_between_vendors.pickle', 'wb') as handle:\n",
    "    pickle.dump(vendor_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dabff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vendor = [vendor1 for (vendor1,vendor2) in vendor_dict.keys()]\n",
    "y_vendor = [vendor2 for (vendor1,vendor2) in vendor_dict.keys()]\n",
    "cosine_similarity = [value for value in vendor_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ec7d18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12996025, 12996025, 12996025)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_vendor), len(y_vendor), len(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c44bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data={'vendors':x_vendor, 'closest vendors':y_vendor, 'similarity':cosine_similarity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0fd195",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_opts = dict(method='zip', archive_name='../data/vendor_similarity.csv')  \n",
    "df.to_csv('../data/vendor_similarity.zip', index=False, compression=compression_opts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec47104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62e1f63e",
   "metadata": {},
   "source": [
    "# Loading the saved dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bbfda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3dcc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/vendor_similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc7eb15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendors</th>\n",
       "      <th>closest vendors</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>debuyerking</td>\n",
       "      <td>debuyerking</td>\n",
       "      <td>0.804351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>debuyerking</td>\n",
       "      <td>drunkninja</td>\n",
       "      <td>0.315869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>debuyerking</td>\n",
       "      <td>pckabml</td>\n",
       "      <td>0.151526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>debuyerking</td>\n",
       "      <td>amphetamine</td>\n",
       "      <td>0.048410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>debuyerking</td>\n",
       "      <td>uridol</td>\n",
       "      <td>-0.014853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12996020</th>\n",
       "      <td>hoggs</td>\n",
       "      <td>scooterfan</td>\n",
       "      <td>0.149337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12996021</th>\n",
       "      <td>hoggs</td>\n",
       "      <td>rainbow snail</td>\n",
       "      <td>0.078590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12996022</th>\n",
       "      <td>hoggs</td>\n",
       "      <td>theauconnection</td>\n",
       "      <td>0.182386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12996023</th>\n",
       "      <td>hoggs</td>\n",
       "      <td>thedman62</td>\n",
       "      <td>-0.006573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12996024</th>\n",
       "      <td>hoggs</td>\n",
       "      <td>hoggs</td>\n",
       "      <td>0.738142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12996025 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              vendors  closest vendors  similarity\n",
       "0         debuyerking      debuyerking    0.804351\n",
       "1         debuyerking       drunkninja    0.315869\n",
       "2         debuyerking          pckabml    0.151526\n",
       "3         debuyerking      amphetamine    0.048410\n",
       "4         debuyerking           uridol   -0.014853\n",
       "...               ...              ...         ...\n",
       "12996020        hoggs       scooterfan    0.149337\n",
       "12996021        hoggs    rainbow snail    0.078590\n",
       "12996022        hoggs  theauconnection    0.182386\n",
       "12996023        hoggs        thedman62   -0.006573\n",
       "12996024        hoggs            hoggs    0.738142\n",
       "\n",
       "[12996025 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vendor_list, closest_vendor_list, similarity_list = ([] for i in range(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537f582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
